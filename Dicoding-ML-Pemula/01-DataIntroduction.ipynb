{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Introduction\n",
    "\n",
    "Machine Learning adalah sebuah bidang yang memberi komputer kemampuan untuk belajar tanpa diprogram secara eksplisit\n",
    "\n",
    "## Mengapa Machine Learning?\n",
    "\n",
    "- ML memberikan powerful tool untuk mempelajari pola suatu data\n",
    "- ML lebih simpel untuk solve complicated problem dibandingkan pemrograman tradisional\n",
    "\n",
    "## Jenis-jenis Machine Learning\n",
    "\n",
    "- Supervised learning, dataset yang digunakan memiliki label. Label adalah tag atau pengenal dari sebuah data\n",
    "- Unsupervised learning, dataset yang digunakan tidak memiliki label. Model unsupervised melakukan belajar sendiri untuk melabeli atau mengelompokkan data\n",
    "- Semi Supervised merupakan gabungan dari supervised learning dan unsupervised learning\n",
    "- Reinforcement Learning adalah model yang belajar menggunakan sistem reward dan penalties.\n",
    "\n",
    "## Data Processing\n",
    "\n",
    "Tahap paling awal dalam setiap proyek ML. Pada tahap ini data akan diambil dari sumber tertentu, dimasukkan pada suatu environment, dan diproses agar bisa diolah oleh model machine learning\n",
    "\n",
    "### Pandas Library\n",
    "\n",
    "Salah satu library yang paling populer untuk pengolahan data dalam machine learning\n",
    "Berikut adalah beberapa contoh data yang dapat diolah dengan pandas, CSV,SQL,SPSS,JSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latihan Konversi Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.05     37.37                27.0       3885.0           661.0   \n1    -118.30     34.26                43.0       1510.0           310.0   \n2    -117.81     33.78                27.0       3589.0           507.0   \n3    -118.36     33.82                28.0         67.0            15.0   \n4    -119.67     36.33                19.0       1241.0           244.0   \n\n   population  households  median_income  median_house_value  \n0      1537.0       606.0         6.6085            344700.0  \n1       809.0       277.0         3.5990            176500.0  \n2      1484.0       495.0         5.7934            270500.0  \n3        49.0        11.0         6.1359            330000.0  \n4       850.0       237.0         2.9375             81700.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.05</td>\n      <td>37.37</td>\n      <td>27.0</td>\n      <td>3885.0</td>\n      <td>661.0</td>\n      <td>1537.0</td>\n      <td>606.0</td>\n      <td>6.6085</td>\n      <td>344700.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-118.30</td>\n      <td>34.26</td>\n      <td>43.0</td>\n      <td>1510.0</td>\n      <td>310.0</td>\n      <td>809.0</td>\n      <td>277.0</td>\n      <td>3.5990</td>\n      <td>176500.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-117.81</td>\n      <td>33.78</td>\n      <td>27.0</td>\n      <td>3589.0</td>\n      <td>507.0</td>\n      <td>1484.0</td>\n      <td>495.0</td>\n      <td>5.7934</td>\n      <td>270500.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-118.36</td>\n      <td>33.82</td>\n      <td>28.0</td>\n      <td>67.0</td>\n      <td>15.0</td>\n      <td>49.0</td>\n      <td>11.0</td>\n      <td>6.1359</td>\n      <td>330000.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-119.67</td>\n      <td>36.33</td>\n      <td>19.0</td>\n      <td>1241.0</td>\n      <td>244.0</td>\n      <td>850.0</td>\n      <td>237.0</td>\n      <td>2.9375</td>\n      <td>81700.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.listdir('sample_data')\n",
    "df = pd.read_csv('sample_data/california_housing_test.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "- Cara paling simpel untuk mengatasi skewness adalah dengan menyamakan proporsi kelas mayoritas dengan kelas minoritas\n",
    "- Ada istilah Garbage In - Garbage Out yang berarti hasil dari machine learning akan buruk jika input yang Anda masukkan juga buruk\n",
    "\n",
    "Berikut adalah beberapa hal yang umum yang harus diperhatikan dalam proses data cleaning:\n",
    "\n",
    "1. Konsistensi Format\n",
    "2. Skala Data\n",
    "3. Duplikasi data\n",
    "4. Missing Value\n",
    "5. Skewness, data tidak seimbang\n",
    "\n",
    "<https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Opsional\n",
    "\n",
    "- Outlier Removal, Salah satu cara termudah untuk mengecek apakah terdapat outlier dalam data kita adalah dengan melakukan visualisasi.\n",
    "- Normalisasi adalah mengubah nilai-nilai dari sebuah fitur ke dalam skala yang sama.\n",
    "- Standardization  proses untuk konversi nilai-nilai dari suatu fitur sehingga nilai-nilai tersebut memiliki skala yang sama\n",
    "\n",
    "<https://scikit-learn.org/0.16/modules/generated/sklearn.preprocessing.MinMaxScaler.html>\n",
    "<https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.25806452 0.45454545]\n [1.         1.        ]\n [0.         0.        ]\n [0.08064516 0.13636364]\n [0.16129032 0.27272727]]\n"
    }
   ],
   "source": [
    "# contoh normalisasi data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[12000000, 33], [35000000, 45], [4000000, 23], [6500000, 26], [9000000, 29]]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data)\n",
    "print(scaler.transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.11638732,  0.23521877],\n       [ 1.94277296,  1.80334389],\n       [-0.83261698, -1.07155217],\n       [-0.60879521, -0.67952089],\n       [-0.38497344, -0.28748961]])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# contoh standarisasi data\n",
    "from sklearn import preprocessing\n",
    "data = [[12000000, 33], [35000000, 45], [4000000, 23], [6500000, 26], [9000000, 29]]\n",
    "scaler = preprocessing.StandardScaler().fit(data)\n",
    "data = scaler.transform(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data storage/warehouse \n",
    "\n",
    "- RDBMS\n",
    "- NoSQL\n",
    "- Firebase\n",
    "- Spark\n",
    "- Bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "- Pilihan yang lebih baik adalah dengan membagi dataset menjadi 2 bagian yaitu data training dan data testing\n",
    "- Data testing diambil dengan proporsi tertentu -umumnya 20% dari keseluruhan data- jika jumlah datanya kecil.\n",
    "- Jika ukuran datanya sangat besar seperti 1 juta record, kita dapat mengambil sekitar 10 ribu data saja untuk testing alias sebesar 1% saja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "30"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sklearn menyediakan dataset iris yakni sebuah dataset yang umum digunakan untuk masalah klasifikasi. Dataset ini memiliki jumlah 150 sampel.\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x=iris.data\n",
    "y=iris.target\n",
    "\n",
    "# Train_test_split mengembalikan 4 nilai\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Evaluation\n",
    "\n",
    "- Validation set atau holdout validation adalah bagian dari train set yang dipakai untuk pengujian model pada tahap awal\n",
    "- Jika ukuran validation set-nya terlalu kecil, maka ada kemungkinan kita memilih model yang tidak optimal.\n",
    "- Ketika ukuran validation set terlalu besar, maka sisa data pada train set lebih kecil dari data train set utuh di mana kondisi ini tidak ideal untuk membandingkan model yang berbeda pada data training yang lebih kecil\n",
    "\n",
    "Solusi untuk masalah ini adalah dengan menggunakan Cross Validation.\n",
    "\n",
    "K-Fold Cross Validation atau lebih sering disebut cross validation adalah salah satu teknik yang populer dipakai dalam evaluasi model ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0.96666667 0.96666667 0.9        0.96666667 1.        ]\n"
    }
   ],
   "source": [
    "# Latihan SKLearn Cross Validation Split\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import  tree\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x=iris.data\n",
    "y=iris.target\n",
    "\n",
    "# model machine learning pertama kita yaitu decision tree, menggunakan library scikit learn.\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Cross_val_score mengembalikan nilai berupa larik atau array yang terdiri dari akurasi pengujian setiap fold dari dataset\n",
    "scores = cross_val_score(clf, x, y, cv=5)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tf",
   "display_name": "tf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}